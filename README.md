# Crypto Scalping RL Agent (On Hold)

## **Project Objectives**
Connect to the Binance API and implement BTC-USDT futures trading.

## **Project Theme**
## **Reinforcement Learning-Based Crypto Scalping Bot**
A short-term trading bot for crypto scalping using reinforcement learning. The ultimate goal is to connect to the Binance API and execute futures trading.

This project originated from a few speculations.

In the end, I believe that the price is derived from market microstructure trades and settlements. Furthermore, I think this assumption holds even better as the observation interval for prices becomes shorter.

Additionally, in short-term trading, most observed participants are rule-based automated trading bots, in my opinion.

I saw the potential application of Reinforcement Learning here. I believed that a model trained on data generated by rule-based models could outperform those rule-based models.

-------

### **Data**
The data used is 1-minute candlestick data for Bitcoin from Upbit. Input data includes price and technical indicator data for the last 60 minutes. Each data point utilizes the original price data and its rate of change.

-------

### **Project Assumptions**
- The training uses Upbit's 1-minute candlestick data, assuming the possibility of both long and short positions.
- Leverage settings are flexible.
- The agent places buy/sell orders every minute.
- The price is assumed to be the **(high + low)/2** at which trades are executed.
- The assumption is made that after the agent places buy/sell orders, all orders are executed within the next minute.

### **Version Information**
* Version 1.1: Uses DNN, only utilizes price data.
* Version 1.2: Uses CNN, includes technical indicators.
* Version 1.3: Simplifies actions in version_2.
* Version 1.4: Adds entropy term to the loss value, uses batch normalization.

* **Version 2.0: Change in Agent Direction**
  * **Version 2.0 Algorithm** 
    1. Select Strategy Duration
    2. Simulate Strategy N times (Reward = Sortino Ratio)
    3. Obtain Reward, the Expectation of N times simulated Reward

#### Architecture: [link](https://github.com/donghui-0126/crypto-scalping-RL-Agent/blob/main/Scalping%20bot.svg)

-------
### **Reason for Pause**
- To return the reward in the version1 manner, it is necessary to calculate how much the Sharpe ratio has positively influenced after each trade. However, the method is unclear.
- For version2, it seems more like model-based learning.
- If it's model-based learning, it might be better to study a bit more.
- The thought of using RL to find a strategy.

### **Future Directions**
- Find a way to give appropriate rewards for each action.
- If it's a model-based model, it might be better to use RL for Market Making based on the following papers:
  - [Transformers for Limit Order Books](https://arxiv.org/abs/2003.00130)
  - [World Model](https://arxiv.org/abs/1803.10122)
  - [Transformers are sample-efficient world models](https://arxiv.org/pdf/2209.00588)

### **Project Duration**
* Project Period: 2023/10 ~~ (On Hold)

<br>

#### Future Improvements: Use indicators like VPIN, add technical indicators, normalize data.
